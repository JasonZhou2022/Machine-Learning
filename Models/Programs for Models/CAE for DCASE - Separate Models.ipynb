{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b501e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7ee8cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "\n",
    "\n",
    "# allAnomaly = np.load(\"allAnomaly3.npy\", allow_pickle=True)\n",
    "\n",
    "# test = allNormal[:]\n",
    "# test = test.tolist()\n",
    "\n",
    "# for i in allAnomaly:\n",
    "#     test.append(i)\n",
    "\n",
    "# print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6614ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWindows(data, numWindows):\n",
    "    windows = []\n",
    "    intervalLength = int(430 / numWindows)\n",
    "    for mspec in range(len(data)):\n",
    "        start = 0\n",
    "        end = intervalLength\n",
    "        for interval in range(numWindows):\n",
    "            melspectrogram = data[mspec]\n",
    "            window = melspectrogram[:,start:end]\n",
    "            windows.append(window)\n",
    "            start += intervalLength\n",
    "            end += intervalLength\n",
    "    return windows\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ff01c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeWindows2(data, numWindows):\n",
    "    windows = []\n",
    "    intervalLength = int(430 / numWindows)\n",
    "    for mspec in tqdm(range(len(data))):\n",
    "        start = 0\n",
    "        end = intervalLength\n",
    "        for interval in range(numWindows):\n",
    "            melspectrogram = data[mspec]\n",
    "            window = melspectrogram[:,start:end]\n",
    "            windows.append([window, mspec, interval])\n",
    "            start += intervalLength\n",
    "            end += intervalLength\n",
    "    return windows\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0dc52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25b7bc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential( #starting size: (11,10)\n",
    "            nn.Conv2d(1,32,3), #input: 11, 10 -> 9, 8\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(32,64,3), #input: 4, 4 -> 2,2\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(2) #input: 2, 2 -> 1, 1\n",
    "            #final: 1, 1\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(64, 32, 3, stride=2), #input: 1,1 -> 3,3\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(32, 1, 6, stride=2, output_padding = (1,0)) #input: 3, 3 -> 11,10\n",
    "            \n",
    "            #final: 11,10\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.encoder(x)\n",
    "        x=self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3384b8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55019cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = CAE()\n",
    "\n",
    "# o = test.encoder(torch.from_numpy(windows[0].reshape(-1,1,11,86)).type(torch.float32))\n",
    "# o.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec76d6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c294705e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ed5a1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(BATCHES[0][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6b284ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batched_training_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "689f3971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y = allNormal[:5, 0]\n",
    "# y2 = makeWindows(y, 5)\n",
    "# y2= np.array(y2)\n",
    "# np.random.seed(2345)\n",
    "# np.random.shuffle(y2)\n",
    "# print(y2.dtype)\n",
    "# y2 = torch.from_numpy(y2).type(torch.float32)\n",
    "# print(type(y2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3c2ac14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Shape: 9045, 31\n",
      "Epoch 49, Loss: 3.7697\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "data = np.load(\"dev_fan.npy\")\n",
    "allNormal = np.load(\"dev_test_fanNormal.npy\", allow_pickle=True)\n",
    "\n",
    "#taking a sample size of 10,000\n",
    "#8:2 split (training_data:testing_data ratio)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:] #length of 8000\n",
    "# testing_data = data[8000:10000] #length of 2000\n",
    "\n",
    "windows = makeWindows(training_data, 43) #this makes the window size 11 x 10\n",
    "# testWindows = makeWindows(testing_data, 5)\n",
    "\n",
    "#creating the batches - fan\n",
    "#total windows will be 247,938\n",
    "BATCH_SIZE = 31\n",
    "BATCHES = []\n",
    "\n",
    "copy = windows\n",
    "\n",
    "seed = 0\n",
    "\n",
    "iterations = int((len(copy))/BATCH_SIZE)\n",
    "for i in range(iterations):\n",
    "    random.seed(seed)\n",
    "    start = random.randint(0, len(copy) - BATCH_SIZE)\n",
    "    seed += 1\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = copy[start:end]\n",
    "    BATCHES.append(batch)\n",
    "    copy = copy[:start] + copy[end:] #taking out the batch from the training_data\n",
    "    #to prevent duplicate numbers from appearing across batches\n",
    "   \n",
    "print(len(copy))\n",
    "print(f'Shape: {len(BATCHES)}, {len(BATCHES[0])}')\n",
    "\n",
    "\n",
    "#converting the python list to tensor\n",
    "\n",
    "batched_training_data = np.array(BATCHES)\n",
    "np.random.seed(2345)\n",
    "np.random.shuffle(batched_training_data)\n",
    "batched_training_data = torch.from_numpy(batched_training_data).type(torch.float32)\n",
    "\n",
    "model = CAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "size = 31\n",
    "\n",
    "allNormalCut = allNormal[:31, 0]\n",
    "allNormalCut = makeWindows(allNormalCut, 43)\n",
    "allNormalCut = allNormalCut[:size]\n",
    "allNormalCut = np.array(allNormalCut)\n",
    " \n",
    "NUM_OF_EPOCH = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#outputs = []\n",
    "\n",
    "def test(testX):\n",
    "    testX = torch.from_numpy(testX).type(torch.float32)\n",
    "    outputs = model(testX.view(-1,1,11,10))\n",
    "    val_loss = criterion(outputs.view(-1,1,11,10), testX.view(-1,1,11,10))\n",
    "    return val_loss\n",
    "\n",
    "with open(\"fan_model1.log\", \"a\") as f:\n",
    "    for epoch in range(NUM_OF_EPOCH):\n",
    "        for i in range(len(batched_training_data)):\n",
    "            output = model(batched_training_data[i].view(-1,1,11,10))\n",
    "            loss = criterion(output.view(-1,1,11,10), batched_training_data[i].view(-1,1,11,10))\n",
    "#             train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                vloss = test(allNormalCut)\n",
    "                val_losses.append(vloss)\n",
    "                f.write(f\"fan_1,{round(time.time(),3)},{round(float(loss.item()), 4)},{round(float(vloss),4)}\\n\")\n",
    "    #outputs.append((epoch, output, batched_training_data[i]))\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "#     np.save(\"training_Losses17.npy\", train_losses)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'fan_1.pt'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99c3be32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Shape: 7998, 31\n",
      "Epoch 49, Loss: 4.4284\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "data = np.load(\"dev_pump.npy\")\n",
    "allNormal = np.load(\"dev_test_pumpNormal.npy\", allow_pickle=True)\n",
    "\n",
    "#taking a sample size of 10,000\n",
    "#8:2 split (training_data:testing_data ratio)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:] #length of 8000\n",
    "# testing_data = data[8000:10000] #length of 2000\n",
    "\n",
    "windows = makeWindows(training_data, 43) #this makes the window size 11 x 10\n",
    "# testWindows = makeWindows(testing_data, 5)\n",
    "\n",
    "#creating the batches - fan\n",
    "#total windows will be 247,938\n",
    "BATCH_SIZE = 31\n",
    "BATCHES = []\n",
    "\n",
    "copy = windows\n",
    "\n",
    "seed = 0\n",
    "\n",
    "iterations = int((len(copy))/BATCH_SIZE)\n",
    "for i in range(iterations):\n",
    "    random.seed(seed)\n",
    "    start = random.randint(0, len(copy) - BATCH_SIZE)\n",
    "    seed += 1\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = copy[start:end]\n",
    "    BATCHES.append(batch)\n",
    "    copy = copy[:start] + copy[end:] #taking out the batch from the training_data\n",
    "    #to prevent duplicate numbers from appearing across batches\n",
    "   \n",
    "print(len(copy))\n",
    "print(f'Shape: {len(BATCHES)}, {len(BATCHES[0])}')\n",
    "\n",
    "\n",
    "#converting the python list to tensor\n",
    "\n",
    "batched_training_data = np.array(BATCHES)\n",
    "np.random.seed(2345)\n",
    "np.random.shuffle(batched_training_data)\n",
    "batched_training_data = torch.from_numpy(batched_training_data).type(torch.float32)\n",
    "\n",
    "model = CAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "size = 31\n",
    "\n",
    "allNormalCut = allNormal[:31, 0]\n",
    "allNormalCut = makeWindows(allNormalCut, 43)\n",
    "allNormalCut = allNormalCut[:size]\n",
    "allNormalCut = np.array(allNormalCut)\n",
    "\n",
    "NUM_OF_EPOCH = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#outputs = []\n",
    "\n",
    "def test(testX):\n",
    "    testX = torch.from_numpy(testX).type(torch.float32)\n",
    "    outputs = model(testX.view(-1,1,11,10))\n",
    "    val_loss = criterion(outputs.view(-1,1,11,10), testX.view(-1,1,11,10))\n",
    "    return val_loss\n",
    "\n",
    "with open(\"pump_model1.log\", \"a\") as f:\n",
    "    for epoch in range(NUM_OF_EPOCH):\n",
    "        for i in range(len(batched_training_data)):\n",
    "            output = model(batched_training_data[i].view(-1,1,11,10))\n",
    "            loss = criterion(output.view(-1,1,11,10), batched_training_data[i].view(-1,1,11,10))\n",
    "#             train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                vloss = test(allNormalCut)\n",
    "                val_losses.append(vloss)\n",
    "                f.write(f\"pump_1,{round(time.time(),3)},{round(float(loss.item()), 4)},{round(float(vloss),4)}\\n\")\n",
    "    #outputs.append((epoch, output, batched_training_data[i]))\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "#     np.save(\"training_Losses17.npy\", train_losses)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'pump_1.pt'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f56edc16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n",
      "Shape: 7176, 31\n",
      "Epoch 49, Loss: 4.8440\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "data = np.load(\"dev_slider.npy\")\n",
    "allNormal = np.load(\"dev_test_sliderNormal.npy\", allow_pickle=True)\n",
    "\n",
    "#taking a sample size of 10,000\n",
    "#8:2 split (training_data:testing_data ratio)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:] #length of 8000\n",
    "# testing_data = data[8000:10000] #length of 2000\n",
    "\n",
    "windows = makeWindows(training_data, 43) #this makes the window size 11 x 10\n",
    "# testWindows = makeWindows(testing_data, 5)\n",
    "\n",
    "#creating the batches - fan\n",
    "#total windows will be 247,938\n",
    "BATCH_SIZE = 31\n",
    "BATCHES = []\n",
    "\n",
    "copy = windows\n",
    "\n",
    "seed = 0\n",
    "\n",
    "iterations = int((len(copy))/BATCH_SIZE)\n",
    "for i in range(iterations):\n",
    "    random.seed(seed)\n",
    "    start = random.randint(0, len(copy) - BATCH_SIZE)\n",
    "    seed += 1\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = copy[start:end]\n",
    "    BATCHES.append(batch)\n",
    "    copy = copy[:start] + copy[end:] #taking out the batch from the training_data\n",
    "    #to prevent duplicate numbers from appearing across batches\n",
    "   \n",
    "print(len(copy))\n",
    "print(f'Shape: {len(BATCHES)}, {len(BATCHES[0])}')\n",
    "\n",
    "\n",
    "#converting the python list to tensor\n",
    "\n",
    "batched_training_data = np.array(BATCHES)\n",
    "np.random.seed(2345)\n",
    "np.random.shuffle(batched_training_data)\n",
    "batched_training_data = torch.from_numpy(batched_training_data).type(torch.float32)\n",
    "\n",
    "model = CAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "size = 31\n",
    "\n",
    "allNormalCut = allNormal[:31, 0]\n",
    "allNormalCut = makeWindows(allNormalCut, 43)\n",
    "allNormalCut = allNormalCut[:size]\n",
    "allNormalCut = np.array(allNormalCut)\n",
    "\n",
    "NUM_OF_EPOCH = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#outputs = []\n",
    "\n",
    "def test(testX):\n",
    "    testX = torch.from_numpy(testX).type(torch.float32)\n",
    "    outputs = model(testX.view(-1,1,11,10))\n",
    "    val_loss = criterion(outputs.view(-1,1,11,10), testX.view(-1,1,11,10))\n",
    "    return val_loss\n",
    "\n",
    "with open(\"slider_model1.log\", \"a\") as f:\n",
    "    for epoch in range(NUM_OF_EPOCH):\n",
    "        for i in range(len(batched_training_data)):\n",
    "            output = model(batched_training_data[i].view(-1,1,11,10))\n",
    "            loss = criterion(output.view(-1,1,11,10), batched_training_data[i].view(-1,1,11,10))\n",
    "#             train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                vloss = test(allNormalCut)\n",
    "                val_losses.append(vloss)\n",
    "                f.write(f\"slider_1,{round(time.time(),3)},{round(float(loss.item()), 4)},{round(float(vloss),4)}\\n\")\n",
    "    #outputs.append((epoch, output, batched_training_data[i]))\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "#     np.save(\"training_Losses17.npy\", train_losses)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'slider_1.pt'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d79e10d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Shape: 9709, 31\n",
      "Epoch 49, Loss: 1.9353\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "data = np.load(\"dev_ToyCar.npy\")\n",
    "allNormal = np.load(\"dev_test_ToyCarNormal.npy\", allow_pickle=True)\n",
    "\n",
    "#taking a sample size of 10,000\n",
    "#8:2 split (training_data:testing_data ratio)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:] #length of 8000\n",
    "# testing_data = data[8000:10000] #length of 2000\n",
    "\n",
    "windows = makeWindows(training_data, 43) #this makes the window size 11 x 10\n",
    "# testWindows = makeWindows(testing_data, 5)\n",
    "\n",
    "#creating the batches - fan\n",
    "#total windows will be 247,938\n",
    "BATCH_SIZE = 31\n",
    "BATCHES = []\n",
    "\n",
    "copy = windows\n",
    "\n",
    "seed = 0\n",
    "\n",
    "iterations = int((len(copy))/BATCH_SIZE)\n",
    "for i in range(iterations):\n",
    "    random.seed(seed)\n",
    "    start = random.randint(0, len(copy) - BATCH_SIZE)\n",
    "    seed += 1\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = copy[start:end]\n",
    "    BATCHES.append(batch)\n",
    "    copy = copy[:start] + copy[end:] #taking out the batch from the training_data\n",
    "    #to prevent duplicate numbers from appearing across batches\n",
    "   \n",
    "print(len(copy))\n",
    "print(f'Shape: {len(BATCHES)}, {len(BATCHES[0])}')\n",
    "\n",
    "\n",
    "#converting the python list to tensor\n",
    "\n",
    "batched_training_data = np.array(BATCHES)\n",
    "np.random.seed(2345)\n",
    "np.random.shuffle(batched_training_data)\n",
    "batched_training_data = torch.from_numpy(batched_training_data).type(torch.float32)\n",
    "\n",
    "model = CAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "size = 31\n",
    "\n",
    "allNormalCut = allNormal[:31, 0]\n",
    "allNormalCut = makeWindows(allNormalCut, 43)\n",
    "allNormalCut = allNormalCut[:size]\n",
    "allNormalCut = np.array(allNormalCut)\n",
    "\n",
    "NUM_OF_EPOCH = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#outputs = []\n",
    "\n",
    "def test(testX):\n",
    "    testX = torch.from_numpy(testX).type(torch.float32)\n",
    "    outputs = model(testX.view(-1,1,11,10))\n",
    "    val_loss = criterion(outputs.view(-1,1,11,10), testX.view(-1,1,11,10))\n",
    "    return val_loss\n",
    "\n",
    "with open(\"ToyCar_model1.log\", \"a\") as f:\n",
    "    for epoch in range(NUM_OF_EPOCH):\n",
    "        for i in range(len(batched_training_data)):\n",
    "            output = model(batched_training_data[i].view(-1,1,11,10))\n",
    "            loss = criterion(output.view(-1,1,11,10), batched_training_data[i].view(-1,1,11,10))\n",
    "#             train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                vloss = test(allNormalCut)\n",
    "                val_losses.append(vloss)\n",
    "                f.write(f\"ToyCar_1,{round(time.time(),3)},{round(float(loss.item()), 4)},{round(float(vloss),4)}\\n\")\n",
    "    #outputs.append((epoch, output, batched_training_data[i]))\n",
    "    print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "#     np.save(\"training_Losses17.npy\", train_losses)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'ToyCar_1.pt'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e69d2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "Shape: 8322, 31\n",
      "Epoch 0, Loss: 27.2805\n",
      "Epoch 1, Loss: 24.3156\n",
      "Epoch 2, Loss: 22.1905\n",
      "Epoch 3, Loss: 20.1724\n",
      "Epoch 4, Loss: 18.3250\n",
      "Epoch 5, Loss: 16.5259\n",
      "Epoch 6, Loss: 14.9356\n",
      "Epoch 7, Loss: 13.3953\n",
      "Epoch 8, Loss: 12.0105\n",
      "Epoch 9, Loss: 10.6048\n",
      "Epoch 10, Loss: 9.4082\n",
      "Epoch 11, Loss: 8.3769\n",
      "Epoch 12, Loss: 7.3402\n",
      "Epoch 13, Loss: 6.4659\n",
      "Epoch 14, Loss: 5.7478\n",
      "Epoch 15, Loss: 4.9693\n",
      "Epoch 16, Loss: 4.4370\n",
      "Epoch 17, Loss: 3.8533\n",
      "Epoch 18, Loss: 3.4017\n",
      "Epoch 19, Loss: 3.0488\n",
      "Epoch 20, Loss: 2.7550\n",
      "Epoch 21, Loss: 2.5219\n",
      "Epoch 22, Loss: 2.3365\n",
      "Epoch 23, Loss: 2.1740\n",
      "Epoch 24, Loss: 2.0505\n",
      "Epoch 25, Loss: 1.9727\n",
      "Epoch 26, Loss: 1.9072\n",
      "Epoch 27, Loss: 1.9086\n",
      "Epoch 28, Loss: 1.8365\n",
      "Epoch 29, Loss: 1.8037\n",
      "Epoch 30, Loss: 1.7898\n",
      "Epoch 31, Loss: 1.8032\n",
      "Epoch 32, Loss: 1.7782\n",
      "Epoch 33, Loss: 1.7779\n",
      "Epoch 34, Loss: 1.7735\n",
      "Epoch 35, Loss: 1.7661\n",
      "Epoch 36, Loss: 1.7630\n",
      "Epoch 37, Loss: 1.7561\n",
      "Epoch 38, Loss: 1.7557\n",
      "Epoch 39, Loss: 1.7419\n",
      "Epoch 40, Loss: 1.7363\n",
      "Epoch 41, Loss: 1.7323\n",
      "Epoch 42, Loss: 1.7219\n",
      "Epoch 43, Loss: 1.7123\n",
      "Epoch 44, Loss: 1.7160\n",
      "Epoch 45, Loss: 1.7135\n",
      "Epoch 46, Loss: 1.6967\n",
      "Epoch 47, Loss: 1.6914\n",
      "Epoch 48, Loss: 1.6771\n",
      "Epoch 49, Loss: 1.6737\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "data = np.load(\"dev_Toy_Conveyor.npy\")\n",
    "allNormal = np.load(\"dev_test_ToyConveyorNormal.npy\", allow_pickle=True)\n",
    "\n",
    "#taking a sample size of 10,000\n",
    "#8:2 split (training_data:testing_data ratio)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:] #length of 8000\n",
    "# testing_data = data[8000:10000] #length of 2000\n",
    "\n",
    "windows = makeWindows(training_data, 43) #this makes the window size 11 x 10\n",
    "# testWindows = makeWindows(testing_data, 5)\n",
    "\n",
    "#creating the batches - fan\n",
    "#total windows will be 247,938\n",
    "BATCH_SIZE = 31\n",
    "BATCHES = []\n",
    "\n",
    "copy = windows\n",
    "\n",
    "seed = 0\n",
    "\n",
    "iterations = int((len(copy))/BATCH_SIZE)\n",
    "for i in range(iterations):\n",
    "    random.seed(seed)\n",
    "    start = random.randint(0, len(copy) - BATCH_SIZE)\n",
    "    seed += 1\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = copy[start:end]\n",
    "    BATCHES.append(batch)\n",
    "    copy = copy[:start] + copy[end:] #taking out the batch from the training_data\n",
    "    #to prevent duplicate numbers from appearing across batches\n",
    "   \n",
    "print(len(copy))\n",
    "print(f'Shape: {len(BATCHES)}, {len(BATCHES[0])}')\n",
    "\n",
    "\n",
    "#converting the python list to tensor\n",
    "\n",
    "batched_training_data = np.array(BATCHES)\n",
    "np.random.seed(2345)\n",
    "np.random.shuffle(batched_training_data)\n",
    "batched_training_data = torch.from_numpy(batched_training_data).type(torch.float32)\n",
    "\n",
    "model = CAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "size = 31\n",
    "\n",
    "allNormalCut = allNormal[:31, 0]\n",
    "allNormalCut = makeWindows(allNormalCut, 43)\n",
    "allNormalCut = allNormalCut[:size]\n",
    "allNormalCut = np.array(allNormalCut)\n",
    "\n",
    "NUM_OF_EPOCH = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#outputs = []\n",
    "\n",
    "def test(testX):\n",
    "    testX = torch.from_numpy(testX).type(torch.float32)\n",
    "    outputs = model(testX.view(-1,1,11,10))\n",
    "    val_loss = criterion(outputs.view(-1,1,11,10), testX.view(-1,1,11,10))\n",
    "    return val_loss\n",
    "\n",
    "with open(\"ToyConveyor_model1.log\", \"a\") as f:\n",
    "    for epoch in range(NUM_OF_EPOCH):\n",
    "        for i in range(len(batched_training_data)):\n",
    "            output = model(batched_training_data[i].view(-1,1,11,10))\n",
    "            loss = criterion(output.view(-1,1,11,10), batched_training_data[i].view(-1,1,11,10))\n",
    "#             train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                vloss = test(allNormalCut)\n",
    "                val_losses.append(vloss)\n",
    "                f.write(f\"ToyConveyor_1,{round(time.time(),3)},{round(float(loss.item()), 4)},{round(float(vloss),4)}\\n\")\n",
    "    #outputs.append((epoch, output, batched_training_data[i]))\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "#     np.save(\"training_Losses17.npy\", train_losses)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'ToyConveyor_1.pt'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2fdf64ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "Shape: 8075, 31\n",
      "Epoch 0, Loss: 11.3169\n",
      "Epoch 1, Loss: 8.7331\n",
      "Epoch 2, Loss: 7.5061\n",
      "Epoch 3, Loss: 7.0863\n",
      "Epoch 4, Loss: 6.9181\n",
      "Epoch 5, Loss: 6.5297\n",
      "Epoch 6, Loss: 6.5321\n",
      "Epoch 7, Loss: 6.4034\n",
      "Epoch 8, Loss: 6.2547\n",
      "Epoch 9, Loss: 6.2791\n",
      "Epoch 10, Loss: 6.2331\n",
      "Epoch 11, Loss: 6.1817\n",
      "Epoch 12, Loss: 6.0636\n",
      "Epoch 13, Loss: 6.0336\n",
      "Epoch 14, Loss: 5.9892\n",
      "Epoch 15, Loss: 5.9627\n",
      "Epoch 16, Loss: 5.9352\n",
      "Epoch 17, Loss: 5.8517\n",
      "Epoch 18, Loss: 5.9021\n",
      "Epoch 19, Loss: 5.8999\n",
      "Epoch 20, Loss: 5.9043\n",
      "Epoch 21, Loss: 5.8982\n",
      "Epoch 22, Loss: 5.9227\n",
      "Epoch 23, Loss: 5.9573\n",
      "Epoch 24, Loss: 5.9356\n",
      "Epoch 25, Loss: 5.8924\n",
      "Epoch 26, Loss: 5.9395\n",
      "Epoch 27, Loss: 5.9344\n",
      "Epoch 28, Loss: 5.9453\n",
      "Epoch 29, Loss: 5.9462\n",
      "Epoch 30, Loss: 5.9355\n",
      "Epoch 31, Loss: 5.9283\n",
      "Epoch 32, Loss: 5.9240\n",
      "Epoch 33, Loss: 5.9299\n",
      "Epoch 34, Loss: 5.8908\n",
      "Epoch 35, Loss: 5.8953\n",
      "Epoch 36, Loss: 5.8808\n",
      "Epoch 37, Loss: 5.8771\n",
      "Epoch 38, Loss: 5.8704\n",
      "Epoch 39, Loss: 5.8632\n",
      "Epoch 40, Loss: 5.8337\n",
      "Epoch 41, Loss: 5.8025\n",
      "Epoch 42, Loss: 5.8091\n",
      "Epoch 43, Loss: 5.7896\n",
      "Epoch 44, Loss: 5.7808\n",
      "Epoch 45, Loss: 5.7698\n",
      "Epoch 46, Loss: 5.7688\n",
      "Epoch 47, Loss: 5.7591\n",
      "Epoch 48, Loss: 5.7740\n",
      "Epoch 49, Loss: 5.7472\n"
     ]
    }
   ],
   "source": [
    "#training\n",
    "data = np.load(\"dev_valve.npy\")\n",
    "allNormal = np.load(\"dev_test_valveNormal.npy\", allow_pickle=True)\n",
    "\n",
    "#taking a sample size of 10,000\n",
    "#8:2 split (training_data:testing_data ratio)\n",
    "np.random.seed(1234)\n",
    "np.random.shuffle(data)\n",
    "training_data = data[:] #length of 8000\n",
    "# testing_data = data[8000:10000] #length of 2000\n",
    "\n",
    "windows = makeWindows(training_data, 43) #this makes the window size 11 x 10\n",
    "# testWindows = makeWindows(testing_data, 5)\n",
    "\n",
    "#creating the batches - fan\n",
    "#total windows will be 247,938\n",
    "BATCH_SIZE = 31\n",
    "BATCHES = []\n",
    "\n",
    "copy = windows\n",
    "\n",
    "seed = 0\n",
    "\n",
    "iterations = int((len(copy))/BATCH_SIZE)\n",
    "for i in range(iterations):\n",
    "    random.seed(seed)\n",
    "    start = random.randint(0, len(copy) - BATCH_SIZE)\n",
    "    seed += 1\n",
    "    end = start + BATCH_SIZE\n",
    "    batch = copy[start:end]\n",
    "    BATCHES.append(batch)\n",
    "    copy = copy[:start] + copy[end:] #taking out the batch from the training_data\n",
    "    #to prevent duplicate numbers from appearing across batches\n",
    "   \n",
    "print(len(copy))\n",
    "print(f'Shape: {len(BATCHES)}, {len(BATCHES[0])}')\n",
    "\n",
    "\n",
    "#converting the python list to tensor\n",
    "\n",
    "batched_training_data = np.array(BATCHES)\n",
    "np.random.seed(2345)\n",
    "np.random.shuffle(batched_training_data)\n",
    "batched_training_data = torch.from_numpy(batched_training_data).type(torch.float32)\n",
    "\n",
    "model = CAE()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "\n",
    "size = 31\n",
    "\n",
    "allNormalCut = allNormal[:31, 0]\n",
    "allNormalCut = makeWindows(allNormalCut, 43)\n",
    "allNormalCut = allNormalCut[:size]\n",
    "allNormalCut = np.array(allNormalCut)\n",
    "\n",
    "NUM_OF_EPOCH = 50\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "#outputs = []\n",
    "\n",
    "def test(testX):\n",
    "    testX = torch.from_numpy(testX).type(torch.float32)\n",
    "    outputs = model(testX.view(-1,1,11,10))\n",
    "    val_loss = criterion(outputs.view(-1,1,11,10), testX.view(-1,1,11,10))\n",
    "    return val_loss\n",
    "\n",
    "with open(\"valve_model1.log\", \"a\") as f:\n",
    "    for epoch in range(NUM_OF_EPOCH):\n",
    "        for i in range(len(batched_training_data)):\n",
    "            output = model(batched_training_data[i].view(-1,1,11,10))\n",
    "            loss = criterion(output.view(-1,1,11,10), batched_training_data[i].view(-1,1,11,10))\n",
    "#             train_losses.append(loss.item())\n",
    "        \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "            if i % 50 == 0:\n",
    "                vloss = test(allNormalCut)\n",
    "                val_losses.append(vloss)\n",
    "                f.write(f\"valve_1,{round(time.time(),3)},{round(float(loss.item()), 4)},{round(float(vloss),4)}\\n\")\n",
    "    #outputs.append((epoch, output, batched_training_data[i]))\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "#     np.save(\"training_Losses17.npy\", train_losses)\n",
    "\n",
    "\n",
    "\n",
    "filename = 'valve_1.pt'\n",
    "pickle.dump(model, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b0b12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = pickle.load(open('model_20.pt','rb'))\n",
    "# a = 100\n",
    "# #look at a = 1, it looks very similar\n",
    "# output = model(torch.from_numpy(windows[a].reshape(-1,1,11,10)).type(torch.float32))\n",
    "# # output2 = model(torch.from_numpy(testWindows[2000].reshape(-1,1,11,86)).type(torch.float32))\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# testing = criterion(output.view(-1,1,11,10), torch.from_numpy(windows[a]).type(torch.float32).view(-1,1,11,10))\n",
    "# print(testing.item())\n",
    "\n",
    "# #reconstructed\n",
    "# plt.figure(figsize=(25,10))\n",
    "# librosa.display.specshow(output.view(11,10).detach().numpy(), x_axis = \"time\", y_axis = \"mel\", sr=22050)\n",
    "\n",
    "# plt.colorbar(format=\"%+2.f\")\n",
    "# plt.show()\n",
    "\n",
    "# #actual\n",
    "# plt.figure(figsize=(25,10))\n",
    "# librosa.display.specshow(windows[a].reshape(11,10), x_axis = \"time\", y_axis = \"mel\", sr=22050)\n",
    "\n",
    "# plt.colorbar(format=\"%+2.f\")\n",
    "# plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44f8581",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
